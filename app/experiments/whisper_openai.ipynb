{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "# Translation with segment-level SRT output\n",
    "srt_output = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format='srt',\n",
    "    prompt=(\n",
    "        \"This is a professionally recorded Hindi movie. \"\n",
    "        \"Speakers include men, women, and children. \"\n",
    "        \"Include emotional, poetic, slang, and culturally nuanced expressions. \"\n",
    "        \"Ensure every spoken word is translated accurately without skipping or hallucinating.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open(\"output_translated.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_output)\n",
    "print(\"‚úÖ Saved: output_translated.srt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Step 1: Convert MP3 to 16kHz WAV (mono, PCM) ===\n",
    "def convert_audio_to_wav(input_path, output_path):\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\",\n",
    "            \"-y\",  # Overwrite without asking\n",
    "            \"-i\", input_path,\n",
    "            \"-ac\", \"1\",                # mono audio\n",
    "            \"-ar\", \"16000\",            # 16kHz\n",
    "            \"-c:a\", \"pcm_s16le\",       # 16-bit PCM\n",
    "            output_path\n",
    "        ], check=True)\n",
    "        print(f\"‚úÖ Audio converted: {output_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"‚ùå FFmpeg conversion failed:\", e)\n",
    "        raise\n",
    "\n",
    "# === Step 2: Format time for SRT ===\n",
    "def to_srt_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=seconds)).split(\".\")[0].replace(\".\", \",\").zfill(8)\n",
    "\n",
    "# === Step 3: Convert segments to SRT ===\n",
    "def convert_segments_to_srt(segments):\n",
    "    srt = []\n",
    "    for i, seg in enumerate(segments):\n",
    "        if not seg.text.strip():\n",
    "            continue\n",
    "        srt.append(str(i + 1))\n",
    "        srt.append(f\"{to_srt_time(seg.start)} --> {to_srt_time(seg.end)}\")\n",
    "        srt.append(seg.text.strip())\n",
    "        srt.append(\"\")\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# === Step 4: Run Whisper translation ===\n",
    "def transcribe_and_translate(input_mp3, output_srt, temp_wav=\"temp_output.wav\"):\n",
    "    # 1. Convert audio\n",
    "    convert_audio_to_wav(input_mp3, temp_wav)\n",
    "\n",
    "    # 2. Transcribe using Whisper API\n",
    "    client = OpenAI(api_key=\"\")\n",
    "    with open(temp_wav, \"rb\") as audio_file:\n",
    "        result = client.audio.translations.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"verbose_json\",\n",
    "            prompt=(\n",
    "                \"This is a professionally recorded Hindi movie. \"\n",
    "                \"It contains real conversations, not stock disclaimers. \"\n",
    "                \"Translate all spoken dialogues to natural, fluent English. \"\n",
    "                \"Avoid fictional disclaimers or hallucinated text. \"\n",
    "                \"Speakers include men, women, and children. \"\n",
    "                \"Preserve cultural tone, slang, and emotion. \"\n",
    "                \"Do not invent content. Skip silence, but not real dialogue.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 3. Convert to SRT\n",
    "    srt_data = convert_segments_to_srt(result.segments)\n",
    "    with open(output_srt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(srt_data)\n",
    "    print(f\"‚úÖ Subtitle saved: {output_srt}\")\n",
    "\n",
    "    # 4. Clean up\n",
    "    if os.path.exists(temp_wav):\n",
    "        os.remove(temp_wav)\n",
    "\n",
    "# === Run the full pipeline ===\n",
    "transcribe_and_translate(\n",
    "    input_mp3=\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\",\n",
    "    output_srt=\"output_cleaned.srt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31835f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "def to_srt_time(seconds):\n",
    "    td = datetime.timedelta(seconds=seconds)\n",
    "    return str(td).split(\".\")[0].zfill(8).replace(\".\", \",\")\n",
    "\n",
    "def group_words_to_srt(words, max_duration=3.0):\n",
    "    srt = []\n",
    "    idx = 1\n",
    "    chunk_words = []\n",
    "    chunk_start = None\n",
    "    chunk_end = None\n",
    "\n",
    "    for word in words:\n",
    "        w = word.word\n",
    "        s = word.start\n",
    "        e = word.end\n",
    "\n",
    "        if chunk_start is None:\n",
    "            chunk_start = s\n",
    "        chunk_end = e\n",
    "        chunk_words.append(w)\n",
    "\n",
    "        if chunk_end - chunk_start >= max_duration or w.endswith(('.', '?', '!')):\n",
    "            srt.append(str(idx))\n",
    "            srt.append(f\"{to_srt_time(chunk_start)} --> {to_srt_time(chunk_end)}\")\n",
    "            srt.append(\" \".join(chunk_words))\n",
    "            srt.append(\"\")\n",
    "            idx += 1\n",
    "            chunk_words = []\n",
    "            chunk_start = None\n",
    "\n",
    "    if chunk_words:\n",
    "        srt.append(str(idx))\n",
    "        srt.append(f\"{to_srt_time(chunk_start)} --> {to_srt_time(chunk_end)}\")\n",
    "        srt.append(\" \".join(chunk_words))\n",
    "        srt.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key = \"\")\n",
    "\n",
    "audio_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\"\n",
    "with open(audio_path, \"rb\") as audio_file:\n",
    "    result = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format='verbose_json',\n",
    "        prompt=\"This is a professionally recorded Hindi movie. Capture all speech, slang, emotional tones, background phrases, and ambient speech.\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "# Validate word-level timestamps\n",
    "if not getattr(result, \"words\", None):\n",
    "    raise ValueError(\"‚ùå Word-level timestamps not returned. Ensure timestamp_granularities=['word'] is supported and audio is valid.\")\n",
    "\n",
    "# Generate and save the final SRT\n",
    "srt_text = group_words_to_srt(result.words)\n",
    "with open(\"output_translated_synced.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_text)\n",
    "\n",
    "print(\"‚úÖ SRT saved as 'output_translated_synced.srt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "client = OpenAI(api_key = \"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "# API call\n",
    "# Get detailed word-level output\n",
    "result = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    response_format='verbose_json',\n",
    "    timestamp_granularities=[\"word\"],\n",
    "    prompt=\"This is a professionally recorded Hindi movie. Include all spoken content with emotions, slang, honorifics.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# SRT time formatter\n",
    "def to_srt_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=seconds)).split(\".\")[0].replace(\".\", \",\").zfill(8)\n",
    "\n",
    "# Grouping function\n",
    "def words_to_srt(words, max_duration=5.0):\n",
    "    srt = []\n",
    "    idx = 1\n",
    "    chunk = []\n",
    "    start = None\n",
    "\n",
    "    for word in words:\n",
    "        w = word.word\n",
    "        s = word.start\n",
    "        e = word.end\n",
    "\n",
    "        if start is None:\n",
    "            start = s\n",
    "\n",
    "        chunk.append(w)\n",
    "\n",
    "        if e - start >= max_duration or w.endswith(('.', '!', '?')):\n",
    "            srt.append(str(idx))\n",
    "            srt.append(f\"{to_srt_time(start)} --> {to_srt_time(e)}\")\n",
    "            srt.append(\" \".join(chunk))\n",
    "            srt.append(\"\")\n",
    "            idx += 1\n",
    "            chunk = []\n",
    "            start = None\n",
    "\n",
    "    # Handle leftover words\n",
    "    if chunk:\n",
    "        srt.append(str(idx))\n",
    "        srt.append(f\"{to_srt_time(start)} --> {to_srt_time(e)}\")\n",
    "        srt.append(\" \".join(chunk))\n",
    "        srt.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# Convert & save\n",
    "srt_text = words_to_srt(result.words)\n",
    "with open(\"output_word_level.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_text)\n",
    "\n",
    "print(\"‚úÖ Word-aligned SRT saved as 'output_word_level.srt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts seconds to SRT time format.\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21937f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math , openai , torch\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Any\n",
    "from datetime import timedelta\n",
    "from faster_whisper import WhisperModel\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "prompt = \"This is a professionally recorded Hindi movie. The speakers include men, women, and children. \\\n",
    "    There may be emotional or poetic expressions. Accurately capture all speech including honorifics and slang.\"\n",
    "\n",
    "\n",
    "segments, info = model.transcribe(audio=\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Ahista/audiofiles/ahista_ahista_part4_audio.mp3\", \n",
    "                                  language= 'hi',\n",
    "                                  beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "# for segment in segments:\n",
    "#     print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "# Write the transcription to an SRT file\n",
    "with open(\"test.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, segment in enumerate(segments, start=1):\n",
    "        start_time = format_time(segment.start)\n",
    "        end_time = format_time(segment.end)\n",
    "        text = segment.text.strip()\n",
    "        f.write(f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f488458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math , openai , torch\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Any\n",
    "from datetime import timedelta\n",
    "from faster_whisper import WhisperModel\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "prompt = \"This is a professionally recorded Hindi movie with emotional and poetic expressions.\"\n",
    "segments, _ = model.transcribe(audio = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/Ahista/audiofiles/ahista_ahista_part1_audio.mp3\", \n",
    "                               language= 'hi', beam_size=5, initial_prompt=\"This is a professionally recorded Hindi movie with emotional and poetic expressions.\")\n",
    "segments_L = list(segments)\n",
    "\n",
    "with open(\"test4.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, segment in enumerate(segments_L, start=1):\n",
    "            start_time = format_time(segment.start)\n",
    "            end_time = format_time(segment.end)\n",
    "            text = segment.text.strip()\n",
    "            f.write(f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysrt\n",
    "\n",
    "subs = pysrt.open('/home/csc/Documents/Multilingual-Transcriber/plugins/experiments/test4.srt')\n",
    "\n",
    "for sub in subs:\n",
    "    print(\"Index:\", sub.index)\n",
    "    print(\"Start:\", sub.start)\n",
    "    print(\"End:\", sub.end)\n",
    "    print(\"Text:\", sub.text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c592f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def get_similarity_score(text1, text2):\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "\n",
    "Instructions:\n",
    "- Return your output in JSON format.\n",
    "- Only include two fields: \"score\" (float from 0 to 1), and \"explanation\" (a short sentence).\n",
    "\n",
    "Example output:\n",
    "{{\"score\": 0.85, \"explanation\": \"The texts describe similar concepts using different words.\"}}\n",
    "\n",
    "Text A: {text1}\n",
    "\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    import json\n",
    "    message = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        data = json.loads(message)\n",
    "        return data[\"score\"], data[\"explanation\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse LLM output: {message}\") from e\n",
    "\n",
    "# Example\n",
    "text1 = \"‡§ê‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§Ø‡•á‡§ó‡§æ ‡§Ø‡§π ‡§¶‡•á‡§ñ‡•ã ‡§Ö‡§¨ ‡§Ü‡§™‡§Æ‡§æ‡§ö ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡§æ ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ ‡§µ‡•ã.\"\n",
    "text2 = \"‡§ê‡§∏‡•á ‡§Ü‡§Ø‡•á‡§ó‡§æ ‡§Ø‡§π ‡§¶‡•á‡§ñ‡•ã ‡§Ö‡§¨ ‡§Ü‡§™‡§Æ‡§æ‡§ö ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡§æ ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ ‡§µ‡•ã.\"\n",
    "\n",
    "score, explanation = get_similarity_score(text1, text2)\n",
    "print(\"Score:\", score)\n",
    "print(\"Explanation:\", explanation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbde02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "\n",
    "def get_similarity_score(text1, text2):\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "\n",
    "Instructions:\n",
    "- Return your output in JSON format.\n",
    "- Include exactly two fields: \n",
    "  \"score\" (float between 0 and 1), and \n",
    "  \"explanation\" (a short sentence explaining the similarity).\n",
    "\n",
    "Example output:\n",
    "{{\"score\": 0.85, \"explanation\": \"The texts describe similar concepts using different words.\"}}\n",
    "\n",
    "Text A: {text1}\n",
    "\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message.content.strip()\n",
    "\n",
    "    # üîß Remove Markdown formatting if present\n",
    "    if message.startswith(\"```\"):\n",
    "        message = message.strip(\"`\")  # remove backticks\n",
    "        message = \"\\n\".join(line for line in message.splitlines() if not line.strip().startswith(\"json\"))\n",
    "\n",
    "    try:\n",
    "        data = json.loads(message)\n",
    "        return data[\"score\"], data[\"explanation\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse LLM output: {message}\") from e\n",
    "\n",
    "# üîç Example usage\n",
    "text1 = \"‡§ê‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§Ø‡•á‡§ó‡§æ ‡§Ø‡§π ‡§¶‡•á‡§ñ‡•ã ‡§Ö‡§¨ ‡§Ü‡§™‡§Æ‡§æ‡§ö ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡§æ ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ ‡§µ‡•ã.\"\n",
    "text2 = \"‡§ê‡§∏‡•á ‡§Ü‡§Ø‡•á‡§ó‡§æ ‡§Ø‡§π ‡§¶‡•á‡§ñ‡•ã ‡§Ö‡§¨ ‡§Ü‡§™‡§Æ‡§æ‡§ö ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡§æ ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ ‡§µ‡•ã.\"\n",
    "\n",
    "score, explanation = get_similarity_score(text1, text2)\n",
    "print(\"Score:\", score)\n",
    "print(\"Explanation:\", explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3af2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/Ahista/audiofiles/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "translation = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\",\n",
    "    prompt= \"This is a professionally recorded Hindi movie with emotional and poetic expressions.\"\n",
    ")\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "# Text to translate\n",
    "text_to_translate = \"‡§Ö‡§∞‡•á ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ï‡§π‡§®‡•á ‡§ï‡§æ ‡§µ‡•ã ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ, ‡§µ‡•ã ‡§ú‡•ã ‡§™‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§®‡§µ‡§ú‡•Ä‡§µ‡§® ‡§¨‡•Å‡§°‡§º‡§æ ‡§Ü‡§∂‡•ç‡§∞‡§Æ ‡§π‡•à ‡§®, ‡§µ‡§π‡§æ‡§Å ‡§™‡•á ‡§¨‡•Å‡§°‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•á‡§µ‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡§§‡•Ä ‡§π‡•à, ‡§ñ‡§æ‡§∏ ‡§ï‡§∞ ‡§õ‡•ã‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä\"\n",
    "\n",
    "# Translate the text to Spanish\n",
    "translated_text = translator.translate(text_to_translate, dest='es').text\n",
    "\n",
    "# Print the translated text\n",
    "print(translated_text)  # Output: Hola, mundo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"\" )\n",
    "def translate_text(text, target_language):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # or \"gpt-4\" if you have access\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful translator. Translate everything to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"‡§Ö‡§∞‡•á ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ï‡§π‡§®‡•á ‡§ï‡§æ ‡§µ‡•ã ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ, ‡§µ‡•ã ‡§ú‡•ã ‡§™‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§®‡§µ‡§ú‡•Ä‡§µ‡§® ‡§¨‡•Å‡§°‡§º‡§æ ‡§Ü‡§∂‡•ç‡§∞‡§Æ ‡§π‡•à ‡§®, ‡§µ‡§π‡§æ‡§Å ‡§™‡•á ‡§¨‡•Å‡§°‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•á‡§µ‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡§§‡•Ä ‡§π‡•à, ‡§ñ‡§æ‡§∏ ‡§ï‡§∞ ‡§õ‡•ã‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä\" \n",
    "target_language = \"en\" # Spanish \n",
    "translation = translate_text(text, target_language) \n",
    "print(translation) # ¬°Hola mundo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98586e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54889da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules required\n",
    "import speech_recognition as spr\n",
    "from googletrans import Translator\n",
    "\n",
    "get_sentence = \"‡§Ö‡§∞‡•á ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ï‡§π‡§®‡•á ‡§ï‡§æ ‡§µ‡•ã ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ, ‡§µ‡•ã ‡§ú‡•ã ‡§™‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§®‡§µ‡§ú‡•Ä‡§µ‡§® ‡§¨‡•Å‡§°‡§º‡§æ ‡§Ü‡§∂‡•ç‡§∞‡§Æ ‡§π‡•à ‡§®, ‡§µ‡§π‡§æ‡§Å ‡§™‡•á ‡§¨‡•Å‡§°‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•á‡§µ‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡§§‡•Ä ‡§π‡•à, ‡§ñ‡§æ‡§∏ ‡§ï‡§∞ ‡§õ‡•ã‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä\" \n",
    "#\n",
    "translator = Translator()\n",
    "\n",
    "# Source and target languages\n",
    "from_lang = 'hi'\n",
    "to_lang = 'gu'\n",
    "\n",
    "\n",
    "\n",
    "# Translate the text\n",
    "text_to_translate = translator.translate(get_sentence, src=from_lang, dest=to_lang)\n",
    "translated_text = text_to_translate.text\n",
    "print(translated_text)\n",
    "                \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-translate pysrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def translate_text(text, target_language=\"bhasa\", source_language=\"hi\", api_key=\" \"):\n",
    "    url = \"https://translation.googleapis.com/language/translate/v2\"\n",
    "    params = {\n",
    "        'q': text,\n",
    "        'target': target_language,\n",
    "        'source': source_language,\n",
    "        'key': api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result['data']['translations'][0]['translatedText']\n",
    "    else:\n",
    "        raise Exception(f\"Translation failed: {response.text}\")\n",
    "\n",
    "# Example usage:\n",
    "translated = translate_text(\"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?\", \"id\")  # Hindi to Bahasa Indonesia\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "import os\n",
    "\n",
    "# Set your service account credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \" \"\n",
    "\n",
    "# Initialize Google Translate client\n",
    "translate_client = translate.Client()\n",
    "\n",
    "def translate_text(text, target_language=\"id\"):\n",
    "    # Translate from Hindi ('hi') to target_language (Bahasa Indonesia = \"id\")\n",
    "    result = translate_client.translate(text, target_language=target_language, source_language=\"hi\")\n",
    "    return result['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"\" )\n",
    "\n",
    "\n",
    "def translate_text_gpt(text, target_language):\n",
    "    prompt = f\"Translate the following Hindi text to {target_language}:\\n\\n{text}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage\n",
    "text = \"‡§Ö‡§∞‡•á ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ï‡§π‡§®‡•á ‡§ï‡§æ ‡§µ‡•ã ‡§Æ‡§§‡§≤‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ...\"\n",
    "translated = translate_text_gpt(text, \"Gujarati\")\n",
    "print(translated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b128b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l  =['gu','mr', 'es']\n",
    "for i in range(len(l)):\n",
    "    print(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551671c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To Print all the languages that google\n",
    "# translator supports\n",
    "import googletrans\n",
    "\n",
    "\n",
    "print(googletrans.LANGUAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c009af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_video_info(path):\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-select_streams\", \"v:0\",\n",
    "        \"-show_entries\", \"stream=codec_name,width,height\",\n",
    "        \"-of\", \"json\", path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"ffprobe failed on {path}: {result.stderr}\")\n",
    "    \n",
    "    stream = json.loads(result.stdout)[\"streams\"][0]\n",
    "    return (stream[\"codec_name\"], stream[\"width\"], stream[\"height\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d644208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_video_formats(video_paths, base_dir):\n",
    "    baseline = None\n",
    "    for path in video_paths:\n",
    "        full_path = os.path.join(base_dir, path)\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "        info = get_video_info(full_path)\n",
    "        if baseline is None:\n",
    "            baseline = info\n",
    "        elif info != baseline:\n",
    "            raise ValueError(f\"Incompatible video format: {path} has {info}, expected {baseline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_videos_ffmpeg_fast(video_paths, base_dir, output_path):\n",
    "    validate_video_formats(video_paths, base_dir)\n",
    "\n",
    "    concat_file = os.path.join(base_dir, \"concat_list.txt\")\n",
    "    with open(concat_file, \"w\") as f:\n",
    "        for video in video_paths:\n",
    "            full_path = os.path.join(base_dir, video)\n",
    "            f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", concat_file,\n",
    "        \"-c\", \"copy\", output_path\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg merge failed: {result.stderr}\")\n",
    "\n",
    "    # os.remove(concat_file)\n",
    "    print(f\"Merged video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "import os\n",
    "\n",
    "video_dir = \"/path/to/videos\"\n",
    "video_files = os.listdir(video_dir)\n",
    "\n",
    "# Only include .mp4 files\n",
    "video_files = [f for f in video_files if f.endswith(\".mp4\")]\n",
    "\n",
    "# Apply natural sort\n",
    "sorted_files = natsorted(video_files)\n",
    "\n",
    "# Write to input.txt in correct format\n",
    "with open(\"input.txt\", \"w\") as f:\n",
    "    for filename in sorted_files:\n",
    "        full_path = os.path.join(video_dir, filename)\n",
    "        f.write(f\"file '{full_path}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/subtitled/Bhasa/\"\n",
    "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "output_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/final_merged1.mp4\"\n",
    "\n",
    "merge_videos_ffmpeg_fast(video_files, video_dir, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from natsort import natsorted  # <-- for natural sorting\n",
    "\n",
    "def get_video_info(path):\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-select_streams\", \"v:0\",\n",
    "        \"-show_entries\", \"stream=codec_name,width,height\",\n",
    "        \"-of\", \"json\", path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"ffprobe failed on {path}: {result.stderr}\")\n",
    "    \n",
    "    stream = json.loads(result.stdout)[\"streams\"][0]\n",
    "    return (stream[\"codec_name\"], stream[\"width\"], stream[\"height\"])\n",
    "\n",
    "def validate_video_formats(video_paths, base_dir):\n",
    "    baseline = None\n",
    "    for path in video_paths:\n",
    "        full_path = os.path.join(base_dir, path)\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "        info = get_video_info(full_path)\n",
    "        if baseline is None:\n",
    "            baseline = info\n",
    "        elif info != baseline:\n",
    "            raise ValueError(f\"Incompatible video format: {path} has {info}, expected {baseline}\")\n",
    "\n",
    "def merge_videos_ffmpeg_fast(video_paths, base_dir, output_path):\n",
    "    # Natural sort to ensure correct order like part1, part2, ..., part10\n",
    "    video_paths = natsorted(video_paths)\n",
    "\n",
    "    # Validate formats before merging\n",
    "    validate_video_formats(video_paths, base_dir)\n",
    "\n",
    "    concat_file = os.path.join(base_dir, \"concat_list.txt\")\n",
    "    with open(concat_file, \"w\") as f:\n",
    "        for video in video_paths:\n",
    "            full_path = os.path.join(base_dir, video)\n",
    "            f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", concat_file,\n",
    "        \"-c\", \"copy\", output_path\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg merge failed: {result.stderr}\")\n",
    "\n",
    "    # Optionally delete concat list\n",
    "    # os.remove(concat_file)\n",
    "    print(f\"Merged video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/subtitled/Bhasa/\"\n",
    "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "output_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/final_merged2.mp4\"\n",
    "\n",
    "merge_videos_ffmpeg_fast(video_files, video_dir, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ffmpeg\n",
    "\n",
    "def synchronize_and_embed_subtitles(video_path, subtitle_path, output_path):\n",
    "    \"\"\"\n",
    "    Synchronize and embed subtitles with the video.\n",
    "    \"\"\"\n",
    "    synced_subtitle_path = \"synced_\" + os.path.basename(subtitle_path)\n",
    "\n",
    "    # Step 1: Sync using ffsubsync\n",
    "    subprocess.run([\n",
    "        \"ffsubsync\", video_path,\n",
    "        \"-i\", subtitle_path,\n",
    "        \"-o\", synced_subtitle_path\n",
    "    ], check=True)\n",
    "\n",
    "    # Step 2: Embed subtitle with ffmpeg (no re-encoding)\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
    "        \"-vf\", f\"subtitles={synced_subtitle_path}\",\n",
    "        \"-c:a\", \"copy\",  # avoid re-encoding audio\n",
    "        output_path\n",
    "    ], check=True)\n",
    "\n",
    "    os.remove(synced_subtitle_path)\n",
    "    print(f\"Subtitled video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be765def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import csv\n",
    "import itertools\n",
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # auth handled via env or config\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def parse_srt_stream(path: str | pathlib.Path) -> Iterator[Tuple[int, str]]:\n",
    "    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        idx, lines = None, []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if idx is not None:\n",
    "                    yield idx, \" \".join(lines)\n",
    "                idx, lines = None, []\n",
    "                continue\n",
    "\n",
    "            if idx is None and line.isdigit():\n",
    "                idx = int(line)\n",
    "            elif \"-->\" in line:\n",
    "                continue  # ignore timecodes\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        if idx is not None:\n",
    "            yield idx, \" \".join(lines)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def llm_similarity(text1: str, text2: str, retries=3) -> float:\n",
    "    if not text1.strip() or not text2.strip():\n",
    "        return 0.0  # empty line = no similarity\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "Return only JSON with:\n",
    "  \"score\" (0-1 float) and \"explanation\" (1 sentence).\n",
    "Text A: {text1}\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "            ).choices[0].message.content.strip()\n",
    "\n",
    "            if res.startswith(\"```\"):\n",
    "                res = \"\\n\".join(l for l in res.strip(\"`\").splitlines()\n",
    "                                if not l.lstrip().startswith(\"json\"))\n",
    "\n",
    "            score = json.loads(res).get(\"score\", 0.0)\n",
    "            return float(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[retry {attempt+1}] LLM error: {e}\")\n",
    "            time.sleep(1)\n",
    "    return 0.0\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def validate_pair_streamed(src_file: str, tgt_file: str, out_csv: str):\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow([\"file_src\", \"file_tgt\", \"index\", \"src_text\", \"tgt_text\", \"similarity\"])\n",
    "\n",
    "        src_iter = parse_srt_stream(src_file)\n",
    "        tgt_iter = parse_srt_stream(tgt_file)\n",
    "\n",
    "        for (i1, t1), (i2, t2) in itertools.zip_longest(src_iter, tgt_iter, fillvalue=(None, \"\")):\n",
    "            score = llm_similarity(t1, t2)\n",
    "            writer.writerow([\n",
    "                pathlib.Path(src_file).name,\n",
    "                pathlib.Path(tgt_file).name,\n",
    "                i1 if i1 is not None else i2,\n",
    "                t1,\n",
    "                t2,\n",
    "                round(score, 4)\n",
    "            ])\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def validate_batch_streamed(pairs: list[Tuple[str, str]], output_dir=\"output_csvs\"):\n",
    "    pathlib.Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    for src, tgt in pairs:\n",
    "        out_file = pathlib.Path(output_dir) / f\"{pathlib.Path(src).stem}__vs__{pathlib.Path(tgt).stem}.csv\"\n",
    "        print(f\"‚ñ∂ Processing: {src} vs {tgt} ‚Üí {out_file.name}\")\n",
    "        validate_pair_streamed(src, tgt, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from core.logging import SingletonLogger, log_exceptions\n",
    "from config.settings import get_settings\n",
    "from models import format_time, translate_text_openai, translate_text_google, wrap_text\n",
    "from utils.language_const import LANGUAGES\n",
    "\n",
    "\n",
    "class Transcribe(ABC):\n",
    "    @abstractmethod\n",
    "    def AudioTranscriptiontoFile(self, model, inputpath: str, languagestoconvert: list, outputfolder: str, outputpath: str, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AudioTranscriptor(Transcribe):\n",
    "    def __init__(self):\n",
    "        self.logger = SingletonLogger.getInstance().logger\n",
    "        self.settings = get_settings()\n",
    "\n",
    "    @log_exceptions(\"Failed during audio transcription\")\n",
    "    def AudioTranscriptiontoFile(self, model, inputpath: str, languagestoconvert: list, outputfolder: str, outputpath: str, *args, **kwargs):\n",
    "        self.logger.info(\"Starting base transcription\")\n",
    "        segments, info = model.transcribe(audio=inputpath, language='hi', beam_size=5)\n",
    "        self.logger.info(f\"Detected language '{info.language}' with probability {info.language_probability:.2f}\")\n",
    "\n",
    "        segments_L = list(segments)\n",
    "        basepath = f\"{outputfolder}Base/{outputpath}\"\n",
    "\n",
    "        with open(basepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            index = 1\n",
    "            for segment in segments_L:\n",
    "                wrapped_lines = wrap_text(segment.text.strip(), max_words=15)\n",
    "                total_lines = len(wrapped_lines) or 1\n",
    "                segment_duration = segment.end - segment.start\n",
    "                duration_per_line = segment_duration / total_lines\n",
    "\n",
    "                for i, line in enumerate(wrapped_lines):\n",
    "                    line_start = segment.start + i * duration_per_line\n",
    "                    line_end = line_start + duration_per_line\n",
    "                    f.write(f\"{index}\\n{format_time(line_start)} --> {format_time(line_end)}\\n{line}\\n\\n\")\n",
    "                    index += 1\n",
    "\n",
    "        self.logger.info(f\"Base SRT file saved at {basepath}\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        def process_language(to_lang):\n",
    "            lang_code = LANGUAGES[to_lang]\n",
    "            subfolder = {\n",
    "                'ms': \"Malay\", 'id': \"Bhasa\", 'bho': \"Bhojpuri\", 'gu': \"Gujarati\",\n",
    "                'mr': \"Marathi\", 'kn': \"Kannada\", 'ml': \"Malayalam\",\n",
    "                'ta': \"Tamil\", 'es': \"Spanish\"\n",
    "            }.get(lang_code, \"Translated\")\n",
    "\n",
    "            basepath_lan = f\"{outputfolder}{subfolder}/{outputpath}\"\n",
    "\n",
    "            with open(basepath_lan, \"w\", encoding=\"utf-8\") as fp:\n",
    "                index = 1\n",
    "                for segment in segments_L:\n",
    "                    text = segment.text.strip()\n",
    "                    translated = (\n",
    "                        translate_text_google(text, lang_code)\n",
    "                        if lang_code in (\"bho\", \"id\")\n",
    "                        else translate_text_openai(text, lang_code)\n",
    "                    )\n",
    "                    wrapped_lines = wrap_text(translated, max_words=15)\n",
    "                    line_count = len(wrapped_lines) or 1\n",
    "                    seg_duration = segment.end - segment.start\n",
    "                    slice_len = seg_duration / line_count\n",
    "\n",
    "                    for i, line in enumerate(wrapped_lines):\n",
    "                        line_start = segment.start + i * slice_len\n",
    "                        line_end = line_start + slice_len\n",
    "                        fp.write(f\"{index}\\n{format_time(line_start)} --> {format_time(line_end)}\\n{line}\\n\\n\")\n",
    "                        index += 1\n",
    "\n",
    "            self.logger.info(f\"SRT for {to_lang} saved at {basepath_lan}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Run translation in threads\n",
    "        with ThreadPoolExecutor(max_workers=len(languagestoconvert)) as executor:\n",
    "            executor.map(process_language, languagestoconvert)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            self.logger.info(\"CUDA memory cache cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fad23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61b550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a7e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcriber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
