{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "# Translation with segment-level SRT output\n",
    "srt_output = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format='srt',\n",
    "    prompt=(\n",
    "        \"This is a professionally recorded Hindi movie. \"\n",
    "        \"Speakers include men, women, and children. \"\n",
    "        \"Include emotional, poetic, slang, and culturally nuanced expressions. \"\n",
    "        \"Ensure every spoken word is translated accurately without skipping or hallucinating.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open(\"output_translated.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_output)\n",
    "print(\"✅ Saved: output_translated.srt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Step 1: Convert MP3 to 16kHz WAV (mono, PCM) ===\n",
    "def convert_audio_to_wav(input_path, output_path):\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\",\n",
    "            \"-y\",  # Overwrite without asking\n",
    "            \"-i\", input_path,\n",
    "            \"-ac\", \"1\",                # mono audio\n",
    "            \"-ar\", \"16000\",            # 16kHz\n",
    "            \"-c:a\", \"pcm_s16le\",       # 16-bit PCM\n",
    "            output_path\n",
    "        ], check=True)\n",
    "        print(f\"✅ Audio converted: {output_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"❌ FFmpeg conversion failed:\", e)\n",
    "        raise\n",
    "\n",
    "# === Step 2: Format time for SRT ===\n",
    "def to_srt_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=seconds)).split(\".\")[0].replace(\".\", \",\").zfill(8)\n",
    "\n",
    "# === Step 3: Convert segments to SRT ===\n",
    "def convert_segments_to_srt(segments):\n",
    "    srt = []\n",
    "    for i, seg in enumerate(segments):\n",
    "        if not seg.text.strip():\n",
    "            continue\n",
    "        srt.append(str(i + 1))\n",
    "        srt.append(f\"{to_srt_time(seg.start)} --> {to_srt_time(seg.end)}\")\n",
    "        srt.append(seg.text.strip())\n",
    "        srt.append(\"\")\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# === Step 4: Run Whisper translation ===\n",
    "def transcribe_and_translate(input_mp3, output_srt, temp_wav=\"temp_output.wav\"):\n",
    "    # 1. Convert audio\n",
    "    convert_audio_to_wav(input_mp3, temp_wav)\n",
    "\n",
    "    # 2. Transcribe using Whisper API\n",
    "    client = OpenAI(api_key=\"\")\n",
    "    with open(temp_wav, \"rb\") as audio_file:\n",
    "        result = client.audio.translations.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"verbose_json\",\n",
    "            prompt=(\n",
    "                \"This is a professionally recorded Hindi movie. \"\n",
    "                \"It contains real conversations, not stock disclaimers. \"\n",
    "                \"Translate all spoken dialogues to natural, fluent English. \"\n",
    "                \"Avoid fictional disclaimers or hallucinated text. \"\n",
    "                \"Speakers include men, women, and children. \"\n",
    "                \"Preserve cultural tone, slang, and emotion. \"\n",
    "                \"Do not invent content. Skip silence, but not real dialogue.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 3. Convert to SRT\n",
    "    srt_data = convert_segments_to_srt(result.segments)\n",
    "    with open(output_srt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(srt_data)\n",
    "    print(f\"✅ Subtitle saved: {output_srt}\")\n",
    "\n",
    "    # 4. Clean up\n",
    "    if os.path.exists(temp_wav):\n",
    "        os.remove(temp_wav)\n",
    "\n",
    "# === Run the full pipeline ===\n",
    "transcribe_and_translate(\n",
    "    input_mp3=\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\",\n",
    "    output_srt=\"output_cleaned.srt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31835f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "def to_srt_time(seconds):\n",
    "    td = datetime.timedelta(seconds=seconds)\n",
    "    return str(td).split(\".\")[0].zfill(8).replace(\".\", \",\")\n",
    "\n",
    "def group_words_to_srt(words, max_duration=3.0):\n",
    "    srt = []\n",
    "    idx = 1\n",
    "    chunk_words = []\n",
    "    chunk_start = None\n",
    "    chunk_end = None\n",
    "\n",
    "    for word in words:\n",
    "        w = word.word\n",
    "        s = word.start\n",
    "        e = word.end\n",
    "\n",
    "        if chunk_start is None:\n",
    "            chunk_start = s\n",
    "        chunk_end = e\n",
    "        chunk_words.append(w)\n",
    "\n",
    "        if chunk_end - chunk_start >= max_duration or w.endswith(('.', '?', '!')):\n",
    "            srt.append(str(idx))\n",
    "            srt.append(f\"{to_srt_time(chunk_start)} --> {to_srt_time(chunk_end)}\")\n",
    "            srt.append(\" \".join(chunk_words))\n",
    "            srt.append(\"\")\n",
    "            idx += 1\n",
    "            chunk_words = []\n",
    "            chunk_start = None\n",
    "\n",
    "    if chunk_words:\n",
    "        srt.append(str(idx))\n",
    "        srt.append(f\"{to_srt_time(chunk_start)} --> {to_srt_time(chunk_end)}\")\n",
    "        srt.append(\" \".join(chunk_words))\n",
    "        srt.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key = \"\")\n",
    "\n",
    "audio_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\"\n",
    "with open(audio_path, \"rb\") as audio_file:\n",
    "    result = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format='verbose_json',\n",
    "        prompt=\"This is a professionally recorded Hindi movie. Capture all speech, slang, emotional tones, background phrases, and ambient speech.\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "# Validate word-level timestamps\n",
    "if not getattr(result, \"words\", None):\n",
    "    raise ValueError(\"❌ Word-level timestamps not returned. Ensure timestamp_granularities=['word'] is supported and audio is valid.\")\n",
    "\n",
    "# Generate and save the final SRT\n",
    "srt_text = group_words_to_srt(result.words)\n",
    "with open(\"output_translated_synced.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_text)\n",
    "\n",
    "print(\"✅ SRT saved as 'output_translated_synced.srt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "client = OpenAI(api_key = \"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/ahista_ahista/audio_files/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "# API call\n",
    "# Get detailed word-level output\n",
    "result = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    response_format='verbose_json',\n",
    "    timestamp_granularities=[\"word\"],\n",
    "    prompt=\"This is a professionally recorded Hindi movie. Include all spoken content with emotions, slang, honorifics.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# SRT time formatter\n",
    "def to_srt_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=seconds)).split(\".\")[0].replace(\".\", \",\").zfill(8)\n",
    "\n",
    "# Grouping function\n",
    "def words_to_srt(words, max_duration=5.0):\n",
    "    srt = []\n",
    "    idx = 1\n",
    "    chunk = []\n",
    "    start = None\n",
    "\n",
    "    for word in words:\n",
    "        w = word.word\n",
    "        s = word.start\n",
    "        e = word.end\n",
    "\n",
    "        if start is None:\n",
    "            start = s\n",
    "\n",
    "        chunk.append(w)\n",
    "\n",
    "        if e - start >= max_duration or w.endswith(('.', '!', '?')):\n",
    "            srt.append(str(idx))\n",
    "            srt.append(f\"{to_srt_time(start)} --> {to_srt_time(e)}\")\n",
    "            srt.append(\" \".join(chunk))\n",
    "            srt.append(\"\")\n",
    "            idx += 1\n",
    "            chunk = []\n",
    "            start = None\n",
    "\n",
    "    # Handle leftover words\n",
    "    if chunk:\n",
    "        srt.append(str(idx))\n",
    "        srt.append(f\"{to_srt_time(start)} --> {to_srt_time(e)}\")\n",
    "        srt.append(\" \".join(chunk))\n",
    "        srt.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(srt)\n",
    "\n",
    "# Convert & save\n",
    "srt_text = words_to_srt(result.words)\n",
    "with open(\"output_word_level.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_text)\n",
    "\n",
    "print(\"✅ Word-aligned SRT saved as 'output_word_level.srt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts seconds to SRT time format.\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21937f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math , openai , torch\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Any\n",
    "from datetime import timedelta\n",
    "from faster_whisper import WhisperModel\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "prompt = \"This is a professionally recorded Hindi movie. The speakers include men, women, and children. \\\n",
    "    There may be emotional or poetic expressions. Accurately capture all speech including honorifics and slang.\"\n",
    "\n",
    "\n",
    "segments, info = model.transcribe(audio=\"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Ahista/audiofiles/ahista_ahista_part4_audio.mp3\", \n",
    "                                  language= 'hi',\n",
    "                                  beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "# for segment in segments:\n",
    "#     print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "# Write the transcription to an SRT file\n",
    "with open(\"test.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, segment in enumerate(segments, start=1):\n",
    "        start_time = format_time(segment.start)\n",
    "        end_time = format_time(segment.end)\n",
    "        text = segment.text.strip()\n",
    "        f.write(f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f488458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math , openai , torch\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Any\n",
    "from datetime import timedelta\n",
    "from faster_whisper import WhisperModel\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "prompt = \"This is a professionally recorded Hindi movie with emotional and poetic expressions.\"\n",
    "segments, _ = model.transcribe(audio = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/Ahista/audiofiles/ahista_ahista_part1_audio.mp3\", \n",
    "                               language= 'hi', beam_size=5, initial_prompt=\"This is a professionally recorded Hindi movie with emotional and poetic expressions.\")\n",
    "segments_L = list(segments)\n",
    "\n",
    "with open(\"test4.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, segment in enumerate(segments_L, start=1):\n",
    "            start_time = format_time(segment.start)\n",
    "            end_time = format_time(segment.end)\n",
    "            text = segment.text.strip()\n",
    "            f.write(f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysrt\n",
    "\n",
    "subs = pysrt.open('/home/csc/Documents/Multilingual-Transcriber/plugins/experiments/test4.srt')\n",
    "\n",
    "for sub in subs:\n",
    "    print(\"Index:\", sub.index)\n",
    "    print(\"Start:\", sub.start)\n",
    "    print(\"End:\", sub.end)\n",
    "    print(\"Text:\", sub.text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c592f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def get_similarity_score(text1, text2):\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "\n",
    "Instructions:\n",
    "- Return your output in JSON format.\n",
    "- Only include two fields: \"score\" (float from 0 to 1), and \"explanation\" (a short sentence).\n",
    "\n",
    "Example output:\n",
    "{{\"score\": 0.85, \"explanation\": \"The texts describe similar concepts using different words.\"}}\n",
    "\n",
    "Text A: {text1}\n",
    "\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    import json\n",
    "    message = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        data = json.loads(message)\n",
    "        return data[\"score\"], data[\"explanation\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse LLM output: {message}\") from e\n",
    "\n",
    "# Example\n",
    "text1 = \"ऐसे नहीं आयेगा यह देखो अब आपमाच शुरू होना आ जाएगा वो.\"\n",
    "text2 = \"ऐसे आयेगा यह देखो अब आपमाच शुरू होना आ जाएगा वो.\"\n",
    "\n",
    "score, explanation = get_similarity_score(text1, text2)\n",
    "print(\"Score:\", score)\n",
    "print(\"Explanation:\", explanation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbde02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "\n",
    "def get_similarity_score(text1, text2):\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "\n",
    "Instructions:\n",
    "- Return your output in JSON format.\n",
    "- Include exactly two fields: \n",
    "  \"score\" (float between 0 and 1), and \n",
    "  \"explanation\" (a short sentence explaining the similarity).\n",
    "\n",
    "Example output:\n",
    "{{\"score\": 0.85, \"explanation\": \"The texts describe similar concepts using different words.\"}}\n",
    "\n",
    "Text A: {text1}\n",
    "\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 🔧 Remove Markdown formatting if present\n",
    "    if message.startswith(\"```\"):\n",
    "        message = message.strip(\"`\")  # remove backticks\n",
    "        message = \"\\n\".join(line for line in message.splitlines() if not line.strip().startswith(\"json\"))\n",
    "\n",
    "    try:\n",
    "        data = json.loads(message)\n",
    "        return data[\"score\"], data[\"explanation\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse LLM output: {message}\") from e\n",
    "\n",
    "# 🔍 Example usage\n",
    "text1 = \"ऐसे नहीं आयेगा यह देखो अब आपमाच शुरू होना आ जाएगा वो.\"\n",
    "text2 = \"ऐसे आयेगा यह देखो अब आपमाच शुरू होना आ जाएगा वो.\"\n",
    "\n",
    "score, explanation = get_similarity_score(text1, text2)\n",
    "print(\"Score:\", score)\n",
    "print(\"Explanation:\", explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3af2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "audio_file = open(\"/home/csc/Documents/Multilingual-Transcriber/shared_data/Ahista/audiofiles/ahista_ahista_part1_audio.mp3\", \"rb\")\n",
    "\n",
    "translation = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\",\n",
    "    prompt= \"This is a professionally recorded Hindi movie with emotional and poetic expressions.\"\n",
    ")\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "# Text to translate\n",
    "text_to_translate = \"अरे मैंने कहने का वो मतलब नहीं था, वो जो पास में नवजीवन बुड़ा आश्रम है न, वहाँ पे बुड़ों की सेवा करने के लिए लोगों की जरूरत पड़ती है, खास कर छोरियों की\"\n",
    "\n",
    "# Translate the text to Spanish\n",
    "translated_text = translator.translate(text_to_translate, dest='es').text\n",
    "\n",
    "# Print the translated text\n",
    "print(translated_text)  # Output: Hola, mundo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"\" )\n",
    "def translate_text(text, target_language):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # or \"gpt-4\" if you have access\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful translator. Translate everything to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"अरे मैंने कहने का वो मतलब नहीं था, वो जो पास में नवजीवन बुड़ा आश्रम है न, वहाँ पे बुड़ों की सेवा करने के लिए लोगों की जरूरत पड़ती है, खास कर छोरियों की\" \n",
    "target_language = \"en\" # Spanish \n",
    "translation = translate_text(text, target_language) \n",
    "print(translation) # ¡Hola mundo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98586e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54889da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules required\n",
    "import speech_recognition as spr\n",
    "from googletrans import Translator\n",
    "\n",
    "get_sentence = \"अरे मैंने कहने का वो मतलब नहीं था, वो जो पास में नवजीवन बुड़ा आश्रम है न, वहाँ पे बुड़ों की सेवा करने के लिए लोगों की जरूरत पड़ती है, खास कर छोरियों की\" \n",
    "#\n",
    "translator = Translator()\n",
    "\n",
    "# Source and target languages\n",
    "from_lang = 'hi'\n",
    "to_lang = 'gu'\n",
    "\n",
    "\n",
    "\n",
    "# Translate the text\n",
    "text_to_translate = translator.translate(get_sentence, src=from_lang, dest=to_lang)\n",
    "translated_text = text_to_translate.text\n",
    "print(translated_text)\n",
    "                \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-translate pysrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def translate_text(text, target_language=\"bhasa\", source_language=\"hi\", api_key=\" \"):\n",
    "    url = \"https://translation.googleapis.com/language/translate/v2\"\n",
    "    params = {\n",
    "        'q': text,\n",
    "        'target': target_language,\n",
    "        'source': source_language,\n",
    "        'key': api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result['data']['translations'][0]['translatedText']\n",
    "    else:\n",
    "        raise Exception(f\"Translation failed: {response.text}\")\n",
    "\n",
    "# Example usage:\n",
    "translated = translate_text(\"आप कैसे हैं?\", \"id\")  # Hindi to Bahasa Indonesia\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "import os\n",
    "\n",
    "# Set your service account credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \" \"\n",
    "\n",
    "# Initialize Google Translate client\n",
    "translate_client = translate.Client()\n",
    "\n",
    "def translate_text(text, target_language=\"id\"):\n",
    "    # Translate from Hindi ('hi') to target_language (Bahasa Indonesia = \"id\")\n",
    "    result = translate_client.translate(text, target_language=target_language, source_language=\"hi\")\n",
    "    return result['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"\" )\n",
    "\n",
    "\n",
    "def translate_text_gpt(text, target_language):\n",
    "    prompt = f\"Translate the following Hindi text to {target_language}:\\n\\n{text}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage\n",
    "text = \"अरे मैंने कहने का वो मतलब नहीं था...\"\n",
    "translated = translate_text_gpt(text, \"Gujarati\")\n",
    "print(translated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b128b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l  =['gu','mr', 'es']\n",
    "for i in range(len(l)):\n",
    "    print(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551671c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To Print all the languages that google\n",
    "# translator supports\n",
    "import googletrans\n",
    "\n",
    "\n",
    "print(googletrans.LANGUAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c009af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_video_info(path):\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-select_streams\", \"v:0\",\n",
    "        \"-show_entries\", \"stream=codec_name,width,height\",\n",
    "        \"-of\", \"json\", path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"ffprobe failed on {path}: {result.stderr}\")\n",
    "    \n",
    "    stream = json.loads(result.stdout)[\"streams\"][0]\n",
    "    return (stream[\"codec_name\"], stream[\"width\"], stream[\"height\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d644208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_video_formats(video_paths, base_dir):\n",
    "    baseline = None\n",
    "    for path in video_paths:\n",
    "        full_path = os.path.join(base_dir, path)\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "        info = get_video_info(full_path)\n",
    "        if baseline is None:\n",
    "            baseline = info\n",
    "        elif info != baseline:\n",
    "            raise ValueError(f\"Incompatible video format: {path} has {info}, expected {baseline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_videos_ffmpeg_fast(video_paths, base_dir, output_path):\n",
    "    validate_video_formats(video_paths, base_dir)\n",
    "\n",
    "    concat_file = os.path.join(base_dir, \"concat_list.txt\")\n",
    "    with open(concat_file, \"w\") as f:\n",
    "        for video in video_paths:\n",
    "            full_path = os.path.join(base_dir, video)\n",
    "            f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", concat_file,\n",
    "        \"-c\", \"copy\", output_path\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg merge failed: {result.stderr}\")\n",
    "\n",
    "    # os.remove(concat_file)\n",
    "    print(f\"Merged video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "import os\n",
    "\n",
    "video_dir = \"/path/to/videos\"\n",
    "video_files = os.listdir(video_dir)\n",
    "\n",
    "# Only include .mp4 files\n",
    "video_files = [f for f in video_files if f.endswith(\".mp4\")]\n",
    "\n",
    "# Apply natural sort\n",
    "sorted_files = natsorted(video_files)\n",
    "\n",
    "# Write to input.txt in correct format\n",
    "with open(\"input.txt\", \"w\") as f:\n",
    "    for filename in sorted_files:\n",
    "        full_path = os.path.join(video_dir, filename)\n",
    "        f.write(f\"file '{full_path}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/subtitled/Bhasa/\"\n",
    "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "output_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/final_merged1.mp4\"\n",
    "\n",
    "merge_videos_ffmpeg_fast(video_files, video_dir, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from natsort import natsorted  # <-- for natural sorting\n",
    "\n",
    "def get_video_info(path):\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-select_streams\", \"v:0\",\n",
    "        \"-show_entries\", \"stream=codec_name,width,height\",\n",
    "        \"-of\", \"json\", path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"ffprobe failed on {path}: {result.stderr}\")\n",
    "    \n",
    "    stream = json.loads(result.stdout)[\"streams\"][0]\n",
    "    return (stream[\"codec_name\"], stream[\"width\"], stream[\"height\"])\n",
    "\n",
    "def validate_video_formats(video_paths, base_dir):\n",
    "    baseline = None\n",
    "    for path in video_paths:\n",
    "        full_path = os.path.join(base_dir, path)\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "        info = get_video_info(full_path)\n",
    "        if baseline is None:\n",
    "            baseline = info\n",
    "        elif info != baseline:\n",
    "            raise ValueError(f\"Incompatible video format: {path} has {info}, expected {baseline}\")\n",
    "\n",
    "def merge_videos_ffmpeg_fast(video_paths, base_dir, output_path):\n",
    "    # Natural sort to ensure correct order like part1, part2, ..., part10\n",
    "    video_paths = natsorted(video_paths)\n",
    "\n",
    "    # Validate formats before merging\n",
    "    validate_video_formats(video_paths, base_dir)\n",
    "\n",
    "    concat_file = os.path.join(base_dir, \"concat_list.txt\")\n",
    "    with open(concat_file, \"w\") as f:\n",
    "        for video in video_paths:\n",
    "            full_path = os.path.join(base_dir, video)\n",
    "            f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", concat_file,\n",
    "        \"-c\", \"copy\", output_path\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg merge failed: {result.stderr}\")\n",
    "\n",
    "    # Optionally delete concat list\n",
    "    # os.remove(concat_file)\n",
    "    print(f\"Merged video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/subtitled/Bhasa/\"\n",
    "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "output_path = \"/home/csc/Documents/Multilingual-Transcriber/shared_data/movieslist/Rishtey/final_merged2.mp4\"\n",
    "\n",
    "merge_videos_ffmpeg_fast(video_files, video_dir, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ffmpeg\n",
    "\n",
    "def synchronize_and_embed_subtitles(video_path, subtitle_path, output_path):\n",
    "    \"\"\"\n",
    "    Synchronize and embed subtitles with the video.\n",
    "    \"\"\"\n",
    "    synced_subtitle_path = \"synced_\" + os.path.basename(subtitle_path)\n",
    "\n",
    "    # Step 1: Sync using ffsubsync\n",
    "    subprocess.run([\n",
    "        \"ffsubsync\", video_path,\n",
    "        \"-i\", subtitle_path,\n",
    "        \"-o\", synced_subtitle_path\n",
    "    ], check=True)\n",
    "\n",
    "    # Step 2: Embed subtitle with ffmpeg (no re-encoding)\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
    "        \"-vf\", f\"subtitles={synced_subtitle_path}\",\n",
    "        \"-c:a\", \"copy\",  # avoid re-encoding audio\n",
    "        output_path\n",
    "    ], check=True)\n",
    "\n",
    "    os.remove(synced_subtitle_path)\n",
    "    print(f\"Subtitled video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be765def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import csv\n",
    "import itertools\n",
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # auth handled via env or config\n",
    "\n",
    "# ──────────────────────────────\n",
    "def parse_srt_stream(path: str | pathlib.Path) -> Iterator[Tuple[int, str]]:\n",
    "    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        idx, lines = None, []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if idx is not None:\n",
    "                    yield idx, \" \".join(lines)\n",
    "                idx, lines = None, []\n",
    "                continue\n",
    "\n",
    "            if idx is None and line.isdigit():\n",
    "                idx = int(line)\n",
    "            elif \"-->\" in line:\n",
    "                continue  # ignore timecodes\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        if idx is not None:\n",
    "            yield idx, \" \".join(lines)\n",
    "\n",
    "# ──────────────────────────────\n",
    "def llm_similarity(text1: str, text2: str, retries=3) -> float:\n",
    "    if not text1.strip() or not text2.strip():\n",
    "        return 0.0  # empty line = no similarity\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a function that compares two texts for contextual similarity.\n",
    "Return only JSON with:\n",
    "  \"score\" (0-1 float) and \"explanation\" (1 sentence).\n",
    "Text A: {text1}\n",
    "Text B: {text2}\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "            ).choices[0].message.content.strip()\n",
    "\n",
    "            if res.startswith(\"```\"):\n",
    "                res = \"\\n\".join(l for l in res.strip(\"`\").splitlines()\n",
    "                                if not l.lstrip().startswith(\"json\"))\n",
    "\n",
    "            score = json.loads(res).get(\"score\", 0.0)\n",
    "            return float(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[retry {attempt+1}] LLM error: {e}\")\n",
    "            time.sleep(1)\n",
    "    return 0.0\n",
    "\n",
    "# ──────────────────────────────\n",
    "def validate_pair_streamed(src_file: str, tgt_file: str, out_csv: str):\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow([\"file_src\", \"file_tgt\", \"index\", \"src_text\", \"tgt_text\", \"similarity\"])\n",
    "\n",
    "        src_iter = parse_srt_stream(src_file)\n",
    "        tgt_iter = parse_srt_stream(tgt_file)\n",
    "\n",
    "        for (i1, t1), (i2, t2) in itertools.zip_longest(src_iter, tgt_iter, fillvalue=(None, \"\")):\n",
    "            score = llm_similarity(t1, t2)\n",
    "            writer.writerow([\n",
    "                pathlib.Path(src_file).name,\n",
    "                pathlib.Path(tgt_file).name,\n",
    "                i1 if i1 is not None else i2,\n",
    "                t1,\n",
    "                t2,\n",
    "                round(score, 4)\n",
    "            ])\n",
    "\n",
    "# ──────────────────────────────\n",
    "def validate_batch_streamed(pairs: list[Tuple[str, str]], output_dir=\"output_csvs\"):\n",
    "    pathlib.Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    for src, tgt in pairs:\n",
    "        out_file = pathlib.Path(output_dir) / f\"{pathlib.Path(src).stem}__vs__{pathlib.Path(tgt).stem}.csv\"\n",
    "        print(f\"▶ Processing: {src} vs {tgt} → {out_file.name}\")\n",
    "        validate_pair_streamed(src, tgt, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from core.logging import SingletonLogger, log_exceptions\n",
    "from config.settings import get_settings\n",
    "from models import format_time, translate_text_openai, translate_text_google, wrap_text\n",
    "from utils.language_const import LANGUAGES\n",
    "\n",
    "\n",
    "class Transcribe(ABC):\n",
    "    @abstractmethod\n",
    "    def AudioTranscriptiontoFile(self, model, inputpath: str, languagestoconvert: list, outputfolder: str, outputpath: str, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AudioTranscriptor(Transcribe):\n",
    "    def __init__(self):\n",
    "        self.logger = SingletonLogger.getInstance().logger\n",
    "        self.settings = get_settings()\n",
    "\n",
    "    @log_exceptions(\"Failed during audio transcription\")\n",
    "    def AudioTranscriptiontoFile(self, model, inputpath: str, languagestoconvert: list, outputfolder: str, outputpath: str, *args, **kwargs):\n",
    "        self.logger.info(\"Starting base transcription\")\n",
    "        segments, info = model.transcribe(audio=inputpath, language='hi', beam_size=5)\n",
    "        self.logger.info(f\"Detected language '{info.language}' with probability {info.language_probability:.2f}\")\n",
    "\n",
    "        segments_L = list(segments)\n",
    "        basepath = f\"{outputfolder}Base/{outputpath}\"\n",
    "\n",
    "        with open(basepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            index = 1\n",
    "            for segment in segments_L:\n",
    "                wrapped_lines = wrap_text(segment.text.strip(), max_words=15)\n",
    "                total_lines = len(wrapped_lines) or 1\n",
    "                segment_duration = segment.end - segment.start\n",
    "                duration_per_line = segment_duration / total_lines\n",
    "\n",
    "                for i, line in enumerate(wrapped_lines):\n",
    "                    line_start = segment.start + i * duration_per_line\n",
    "                    line_end = line_start + duration_per_line\n",
    "                    f.write(f\"{index}\\n{format_time(line_start)} --> {format_time(line_end)}\\n{line}\\n\\n\")\n",
    "                    index += 1\n",
    "\n",
    "        self.logger.info(f\"Base SRT file saved at {basepath}\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        def process_language(to_lang):\n",
    "            lang_code = LANGUAGES[to_lang]\n",
    "            subfolder = {\n",
    "                'ms': \"Malay\", 'id': \"Bhasa\", 'bho': \"Bhojpuri\", 'gu': \"Gujarati\",\n",
    "                'mr': \"Marathi\", 'kn': \"Kannada\", 'ml': \"Malayalam\",\n",
    "                'ta': \"Tamil\", 'es': \"Spanish\"\n",
    "            }.get(lang_code, \"Translated\")\n",
    "\n",
    "            basepath_lan = f\"{outputfolder}{subfolder}/{outputpath}\"\n",
    "\n",
    "            with open(basepath_lan, \"w\", encoding=\"utf-8\") as fp:\n",
    "                index = 1\n",
    "                for segment in segments_L:\n",
    "                    text = segment.text.strip()\n",
    "                    translated = (\n",
    "                        translate_text_google(text, lang_code)\n",
    "                        if lang_code in (\"bho\", \"id\")\n",
    "                        else translate_text_openai(text, lang_code)\n",
    "                    )\n",
    "                    wrapped_lines = wrap_text(translated, max_words=15)\n",
    "                    line_count = len(wrapped_lines) or 1\n",
    "                    seg_duration = segment.end - segment.start\n",
    "                    slice_len = seg_duration / line_count\n",
    "\n",
    "                    for i, line in enumerate(wrapped_lines):\n",
    "                        line_start = segment.start + i * slice_len\n",
    "                        line_end = line_start + slice_len\n",
    "                        fp.write(f\"{index}\\n{format_time(line_start)} --> {format_time(line_end)}\\n{line}\\n\\n\")\n",
    "                        index += 1\n",
    "\n",
    "            self.logger.info(f\"SRT for {to_lang} saved at {basepath_lan}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Run translation in threads\n",
    "        with ThreadPoolExecutor(max_workers=len(languagestoconvert)) as executor:\n",
    "            executor.map(process_language, languagestoconvert)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            self.logger.info(\"CUDA memory cache cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fad23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61b550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a7e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcriber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
